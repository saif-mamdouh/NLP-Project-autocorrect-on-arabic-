{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95El_cTPkjDp",
        "outputId": "f2075824-4490-42cf-fc2f-e68952a18718"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xPH4xgqnYvb",
        "outputId": "8c28d6e9-cf29-459e-f338-fa885c999b66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcNH1BCozEoO",
        "outputId": "bafce42d-ba44-4c99-c504-b6a6c5b15c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n",
            "Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/126.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m122.9/126.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtILRn3-zNSd",
        "outputId": "6d7a8cea-55af-48e9-e7d4-2ac59eeb991e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OPa0OXntnJDz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForMaskedLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dooeJHCRnpYL"
      },
      "outputs": [],
      "source": [
        "# Initialize model and tokenizer\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"/content/my_model\")\n",
        "    model = AutoModelForMaskedLM.from_pretrained(\"/content/my_model\")\n",
        "    print(\"Loaded fine-tuned model from /content/my_model\")\n",
        "except:\n",
        "    print(\"Using pretrained model\")\n",
        "    model_name = \"UBC-NLP/MARBERT\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForMaskedLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpinlmwsFG86"
      },
      "outputs": [],
      "source": [
        "# Move model to correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMcxHYhZ8a3n"
      },
      "source": [
        "# **Data preprocessing functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6_cXS6VoROp"
      },
      "outputs": [],
      "source": [
        "def preprocess(sentence: str) -> str:\n",
        "    \"\"\"Enhanced Arabic text preprocessing with additional normalization\"\"\"\n",
        "    sentence = sentence.replace('Ø£', 'Ø§').replace('Ø¥', 'Ø§').replace('Ø¢', 'Ø§')\n",
        "    sentence = re.sub(r'[^\\u0600-\\u06FF\\s]', '', sentence)  # Keep only Arabic characters and spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence).strip()  # Clean up extra spaces\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def data_vocab(dataframe, min_freq=3):\n",
        "    \"\"\"Create vocabulary with frequency filtering\"\"\"\n",
        "    words_freq = Counter()\n",
        "    for text in dataframe['text']:\n",
        "        words_freq.update(text.split())\n",
        "    return {word: freq for word, freq in words_freq.items() if freq >= min_freq}\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenization function for dataset\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev7llNDl-rVU"
      },
      "source": [
        "# **Prediction functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjDDIi0a-GKs"
      },
      "outputs": [],
      "source": [
        "def normalize_hamza(word: str) -> str:\n",
        "    # Normalize common Hamza variants to unify them\n",
        "    word = word.replace(\"Ø£\", \"Ø§\").replace(\"Ø¥\", \"Ø§\").replace(\"Ø¤\", \"Ùˆ\").replace(\"Ø¦\", \"ÙŠ\").replace(\"Ø¡\", \"\")\n",
        "    return word\n",
        "\n",
        "def find_misspellings(text: str, vocab: dict, threshold: float = 0.28) -> list:\n",
        "    \"\"\"Identify potentially misspelled words using MLM probability and additional context\"\"\"\n",
        "    words = text.split()\n",
        "    misspelled_indices = []\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        if word not in vocab and normalize_hamza(word) not in vocab:  # Word not in vocab (may be misspelled)\n",
        "            masked_words = words.copy()\n",
        "            masked_words[i] = tokenizer.mask_token\n",
        "            masked_sentence = \" \".join(masked_words)\n",
        "\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\").to(device)\n",
        "            mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits[0, mask_token_index]\n",
        "                probs = torch.softmax(logits, dim=-1).squeeze()\n",
        "                word_id = tokenizer.encode(word, add_special_tokens=False)\n",
        "                word_prob = torch.mean(probs[word_id]) if word_id else 0\n",
        "\n",
        "            # Use lower threshold to catch more potential errors\n",
        "            if word_prob < threshold:\n",
        "                misspelled_indices.append(i)\n",
        "\n",
        "    return misspelled_indices\n",
        "\n",
        "\n",
        "def generate_masked_sentences(text: str, misspelled_indices: list) -> list:\n",
        "    \"\"\"Generate masked sentences for each misspelled word\"\"\"\n",
        "    words = text.split()\n",
        "    return [\n",
        "        \" \".join(words[:idx] + [tokenizer.mask_token] + words[idx + 1:])\n",
        "        for idx in misspelled_indices\n",
        "    ]\n",
        "\n",
        "\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "def predict(masked_sentence: str, top_k=25) -> list:\n",
        "    \"\"\"Predict top-k masked words from MLM\"\"\"\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\").to(device)\n",
        "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits[0, mask_token_index]\n",
        "    probs = torch.softmax(logits, dim=-1).squeeze()\n",
        "    top_k_tokens = torch.topk(probs, top_k)\n",
        "\n",
        "    predictions = []\n",
        "    for token_id in top_k_tokens.indices:\n",
        "        token = tokenizer.decode([token_id]).strip()\n",
        "        # Ù†ØªØ§ÙƒØ¯ Ø¥Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø¹Ø±Ø¨ÙŠØ© ÙˆÙ„Ù‡Ø§ Ø·ÙˆÙ„ Ù…Ø¹Ù‚ÙˆÙ„\n",
        "        if re.match(r'^[\\u0600-\\u06FF]{2,}$', token):\n",
        "            predictions.append(token)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N1WPUSPFiam",
        "outputId": "ebceeba7-bedb-4c6f-e373-43dcee5f8933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model âœ… ÙˆØ²Ø²Ø§Ø±Ø© [MASK] ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ… â†’ ['Ø§Ù„ØªØ±Ø¨ÙŠØ©', 'Ø§Ù„ØªØ±Ø¨ÙŠÙ‡', 'Ù„Ù„ØªØ±Ø¨ÙŠØ©', 'Ø§Ù„ØµØ­Ù‡', 'Ø§Ù„ØµØ­Ø©', 'ÙˆØ§Ù„ØªØ±Ø¨ÙŠØ©', 'Ø§Ù„Ù†Ù‚Ù„', 'Ø¨Ø§Ù„ØªØ±Ø¨ÙŠØ©', 'Ø·ÙŠØ¨', 'Ø¨Ø§Ù„ØµØ­Ù‡', 'ØªØ±Ø¨ÙŠÙ‡', 'ÙˆØ§Ù„ØµØ­Ù‡', 'ØªØ±Ø¨ÙŠØ©', 'Ø§Ù†Ø§', 'Ø·Ø¨', 'Ø§Ù„Ù…Ø¹Ù„Ù…', 'Ø§Ù„Ø§Ù†ØªØ³Ø§Ø¨', 'Ø§Ù„Ù…Ø¯Ø±Ø³Ù‡', 'Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ†', 'Ø§Ù„ØªØ¹Ù„ÙŠÙ…', 'Ø§Ù„ØªØ¹Ù„Ù…', 'Ø§Ù„Ù…Ù†Ø§Ù‡Ø¬', 'Ù‚ÙŠØ§Ø³', 'Ø¨Ø§Ù„ØµØ­Ø©'] (Expected: Ø§Ù„ØªØ±Ø¨ÙŠØ©)\n",
            "Model âœ… ÙŠÙˆÙ… [MASK] â†’ ['Ø¬Ù…ÙŠÙ„', 'Ø§Ù„Ø¬Ù…Ø¹Ù‡', 'Ù…ÙŠÙ„Ø§Ø¯ÙŠ', 'Ø§Ù„Ø®Ù…ÙŠØ³', 'Ù…Ù…ÙŠØ²', 'Ø§Ù„Ø¬Ù…Ø¹Ø©', 'Ø­Ù„Ùˆ', 'Ù„Ø·ÙŠÙ', 'Ø§Ù„Ø§Ø­Ø¯', 'Ø³Ø¹ÙŠØ¯', 'Ø¬Ù…ÙŠÙŠÙ„', 'Ø¬Ø¯ÙŠØ¯', 'Ø§Ù„Ø³Ø¨Øª', 'Ø§Ù„ØªÙ„Ø§Øª', 'Ø¹Ø¸ÙŠÙ…', 'Ø§Ù„Ù…Ø¹Ù„Ù…', 'Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡', 'Ø§Ù„Ø§Ø±Ø¨Ø¹Ø§Ø¡', 'ØªØ§Ø±ÙŠØ®ÙŠ', 'Ø§Ù„Ø¹Ø¸Ù…Ø§Ø¡', 'Ø­Ø§ÙÙ„', 'Ø¹Ø§Ù„Ù…ÙŠ', 'Ø®Ù…ÙŠØ³', 'Ø§Ù„Ø¹Ù„Ù…'] (Expected: Ø§Ù„Ø³Ø¨Øª)\n",
            "Model âœ… Ø§Ù„Ø·Ù‚Ø³ Ø§Ù„ÙŠÙˆÙ… [MASK] â†’ ['Ø¬Ù…ÙŠÙ„', 'Ø­Ù„Ùˆ', 'Ø¨Ø§Ø±Ø¯', 'Ø±Ø§ÙŠØ¹', 'Ù„Ø·ÙŠÙ', 'Ø­Ø§Ø±', 'Ø®Ø±Ø§ÙÙŠ', 'ØºØ§ÙŠÙ…', 'Ù…Ø®ØªÙ„Ù', 'Ø§Ù…Ø·Ø§Ø±', 'Ø±ÙˆÙˆØ¹Ù‡', 'Ù‡Ù‡', 'Ø±Ø¨ÙŠØ¹ÙŠ', 'Ø­Ø±', 'ØºØ¨Ø§Ø±', 'Ø­Ù„ÙˆÙˆ', 'Ø¬Ù…ÙŠÙŠÙ„', 'Ù…Ù…ØªØ§Ø²', 'Ø±ÙˆØ¹Ù‡', 'Ø¶Ø¨Ø§Ø¨', 'Ø±ÙˆÙˆØ¹Ø©', 'Ù…Ø§Ø·Ø±'] (Expected: Ø­Ø±)\n",
            "Model âœ… Ø§Ù„Ù„ØºØ© [MASK] ØµØ¹Ø¨Ø© â†’ ['Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©', 'Ø§Ù„Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠØ©', 'Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©', 'Ø§Ù„Ø§Ù„Ù…Ø§Ù†ÙŠØ©', 'ØµØ§Ø±Øª', 'Ø§Ù„Ø§Ø³Ø¨Ø§Ù†ÙŠØ©', 'Ø§Ù„ØµÙŠÙ†ÙŠØ©', 'Ø§Ù„ÙØ§Ø±Ø³ÙŠØ©', 'Ø§Ù„ØªØ±ÙƒÙŠØ©', 'Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‡', 'Ø§Ù„Ø§ÙŠØ·Ø§Ù„ÙŠØ©', 'Ø¬Ø¯Ø§', 'Ø§Ù„Ø±ÙˆØ³ÙŠØ©', 'Ø¨Ù‚Øª', 'Ù…Ø±Ø©', 'Ø§Ù„ÙØµØ­Ù‰', 'Ø¹Ù†Ø¯Ùƒ', 'Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ©', 'Ø¯ÙŠ', 'Ø¹Ù†Ø¯Ù‡Ù…', 'Ø§Ø­ÙŠØ§Ù†Ø§', 'Ø§Ù„Ù…ØµØ±ÙŠØ©', 'Ø§Ù„ÙƒØ±Ø¯ÙŠØ©', 'Ø§Ù„Ù„Ø¨Ù†Ø§Ù†ÙŠØ©', 'Ø·Ù„Ø¹Øª'] (Expected: Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)\n",
            "Model âœ… Ø°Ù‡Ø¨ Ù…Ø­Ù…Ø¯ Ø¥Ù„Ù‰ [MASK] â†’ ['Ø§Ù„Ø¬Ø­ÙŠÙ…', 'Ø§Ù„Ù„Ù‡', 'Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰', 'Ø±Ø¨Ù‡', 'Ø¬Ù‡Ù†Ù…', 'Ø§Ù„Ù†ÙˆÙ…', 'Ø§Ù„Ø¬Ù†Ø©', 'Ø§Ù„Ù…Ø³Ø¬Ø¯', 'Ø§Ù„Ù‚Ø¨Ø±', 'Ø§Ù„ØµÙ„Ø§Ø©', 'Ø§Ù„Ù†Ø§Ø±', 'Ø§Ù„Ø³Ù…Ø§Ø¡', 'Ù…Ù†Ø²Ù„Ù‡', 'Ø§Ù„Ø¬Ù†Ù‡', 'Ø§Ù„Ø³Ø¬Ù†', 'Ø§Ù„Ù…Ø¯Ø±Ø³Ø©', 'Ù‚Ø¨Ø±Ù‡', 'Ø§Ù„Ù‚Ù…Ø±', 'Ø¬Ø¯Ù‡', 'ÙƒØ±Ø¨Ù„Ø§Ø¡', 'Ù…Ø­Ù…Ø¯', 'Ù…ØµØ±', 'Ø§Ù„Ù…ÙˆØª', 'Ù„Ù†Ø¯Ù†'] (Expected: Ø§Ù„Ù…Ø¯Ø±Ø³Ø©)\n",
            "Model âœ… Ø£Ù†Ø§ Ø£Ø­Ø¨ [MASK] ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø¡ â†’ ['Ø§Ù„Ù‚Ù‡ÙˆØ©', 'Ø§Ù„Ù‚Ù‡ÙˆÙ‡', 'Ø§Ù„Ø³Ù‡Ø±', 'Ø§Ù„Ù†ÙˆÙ…', 'Ø§Ù„Ø¨Ø­Ø±', 'ØµÙˆØªÙƒ', 'Ø§Ù„ØµØ¨Ø§Ø­', 'Ù‚Ù‡ÙˆØªÙŠ', 'ØªÙˆÙŠØªØ±', 'Ø§Ø¬Ù„Ø³', 'Ø§Ù„Ø¯ÙˆØ§Ù…', 'Ø§Ù„Ù‡Ø¯ÙˆØ¡', 'Ø§Ù„Ù„Ø¹Ø¨', 'Ø§Ù„Ù…Ø·Ø±', 'Ø§Ù„Ø±Ù‚Øµ', 'Ø§Ù„Ù„ÙŠÙ„', 'Ø§Ù†Ø§Ù…', 'Ø§Ù„Ø´Ø§ÙŠ', 'Ø§Ø³Ù‡Ø±', 'Ø§Ù„Ø³ÙØ±', 'Ø§Ù„Ù…Ø´ÙŠ', 'Ø§Ù„ØºØ±ÙˆØ¨', 'Ø§Ø®Ø±Ø¬', 'Ø§Ø³Ù…Ø¹Ù‡Ø§', 'Ø§Ø´ÙˆÙÙƒ'] (Expected: Ø§Ù„Ù„Ø¹Ø¨)\n",
            "Model âŒ Ø§Ù„Ø³ÙŠØ§Ø±Ø© [MASK] ÙÙŠ Ø§Ù„Ø·Ø±ÙŠÙ‚ â†’ ['ÙˆØ§Ù‚ÙØ©', 'ØªØ³ÙŠØ±', 'Ù…ØªÙˆÙ‚ÙØ©', 'Ø§Ù„Ù„ÙŠ', 'ØªÙ…Ø´ÙŠ', 'ÙˆØ§Ù‚ÙÙ‡', 'ÙˆÙ‚ÙØª', 'Ø§Ù„Ø«Ø§Ù†ÙŠØ©', 'Ø§Ù„ÙŠ', 'ØªÙ†ØªØ¸Ø±Ùƒ', 'Ø§Ù„Ø§Ù†', 'Ø´ØºØ§Ù„Ø©', 'ÙƒÙ„Ù‡Ø§', 'Ø²Ø­Ù…Ø©', 'ØªÙ†ØªØ¸Ø±Ù†ÙŠ', 'Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©', 'ØªØ¹Ø·Ù„Øª', 'Ù…Ø§Ø²Ø§Ù„Øª', 'Ø±Ø§Ø­Øª', 'Ø¶Ø§Ø¹Øª', 'Ù…ÙØªÙˆØ­Ø©', 'Ù…Ø§Ø´ÙŠØ©', 'ØªÙˆÙ‚Ù', 'ÙØ§Ø¶ÙŠØ©', 'Ù‚Ø§ÙÙ„Ø©'] (Expected: Ø³Ø±ÙŠØ¹Ø©)\n",
            "Model âœ… ÙƒØ±Ø© [MASK] Ù‡ÙŠ Ø§Ù„Ù…ÙØ¶Ù„Ø© Ù„Ø¯ÙŠ â†’ ['Ø§Ù„Ù‚Ø¯Ù…', 'Ø§Ù„ÙŠØ¯', 'Ø§Ù„Ø³Ù„Ø©', 'Ù‚Ø¯Ù…', 'Ø§Ù„Ø·Ø§ÙŠØ±Ø©', 'Ø§Ù„Ù…Ø§Ø¡', 'Ø§Ù„Ø³Ù„Ù‡', 'Ø§Ù„ØªÙ†Ø³', 'Ø§Ù„Ø«Ù„Ø¬', 'Ù…ÙŠØ³ÙŠ', 'Ø§Ù„Ø·Ø§ÙˆÙ„Ø©', 'Ø³Ù„Ø©', 'Ø§Ù„Ù‡Ø§ØªØ±ÙŠÙƒ', 'ÙŠØ¯', 'Ø§Ù„Ù…Ø·Ø±', 'Ø§Ù„Ø¯Ø¬Ø§Ø¬', 'Ø§Ù„Ø°Ù‡Ø¨ÙŠØ©', 'Ø§Ù„Ø¸Ù‡Ø±', 'Ø§Ù„Ø§Ø¨Ø·Ø§Ù„', 'Ø§Ù„Ø·Ø§ÙŠØ±Ù‡', 'Ø§Ù„Ø§Ø±Ø¬Ù†ØªÙŠÙ†', 'Ø¨Ø±Ø´Ù„ÙˆÙ†Ø©', 'Ø§Ù„Ø§Ù…Ø§Ø±Ø§Øª', 'Ø§Ù„ØµØ§Ù„Ø§Øª'] (Expected: Ø§Ù„Ù‚Ø¯Ù…)\n"
          ]
        }
      ],
      "source": [
        "test_cases = [\n",
        "    (\"ÙˆØ²Ø²Ø§Ø±Ø© [MASK] ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ…\", \"Ø§Ù„ØªØ±Ø¨ÙŠØ©\"),\n",
        "    (\"ÙŠÙˆÙ… [MASK]\", \"Ø§Ù„Ø³Ø¨Øª\"),\n",
        "    (\"Ø§Ù„Ø·Ù‚Ø³ Ø§Ù„ÙŠÙˆÙ… [MASK]\", \"Ø­Ø±\"),\n",
        "    (\"Ø§Ù„Ù„ØºØ© [MASK] ØµØ¹Ø¨Ø©\", \"Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"),\n",
        "    (\"Ø°Ù‡Ø¨ Ù…Ø­Ù…Ø¯ Ø¥Ù„Ù‰ [MASK]\", \"Ø§Ù„Ù…Ø¯Ø±Ø³Ø©\"),\n",
        "    (\"Ø£Ù†Ø§ Ø£Ø­Ø¨ [MASK] ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø¡\", \"Ø§Ù„Ù„Ø¹Ø¨\"),\n",
        "    (\"Ø§Ù„Ø³ÙŠØ§Ø±Ø© [MASK] ÙÙŠ Ø§Ù„Ø·Ø±ÙŠÙ‚\", \"Ø³Ø±ÙŠØ¹Ø©\"),\n",
        "    (\"ÙƒØ±Ø© [MASK] Ù‡ÙŠ Ø§Ù„Ù…ÙØ¶Ù„Ø© Ù„Ø¯ÙŠ\", \"Ø§Ù„Ù‚Ø¯Ù…\")\n",
        "]\n",
        "\n",
        "for sentence, expected in test_cases:\n",
        "    preds = predict(sentence)\n",
        "    is_correct = expected in preds\n",
        "    print(f\"Model {'âœ…' if is_correct else 'âŒ'} {sentence} â†’ {preds} (Expected: {expected})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf4FO_fm-xuU"
      },
      "source": [
        "# **Pipeline function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4s0ZH1X83c_"
      },
      "outputs": [],
      "source": [
        "def pipeline(input_text: str, vocab: dict, verbose: bool = True) -> str:\n",
        "    processed_text = preprocess(input_text)\n",
        "    vocab = data_vocab(df, min_freq=3)\n",
        "    misspelled_indices = find_misspellings(processed_text, vocab)\n",
        "\n",
        "    if not misspelled_indices:\n",
        "        if verbose:\n",
        "            print(\"âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø·Ø§Ø¡ Ø¥Ù…Ù„Ø§Ø¦ÙŠØ© ÙˆØ§Ø¶Ø­Ø©.\")\n",
        "        return processed_text\n",
        "\n",
        "    masked_sentences = generate_masked_sentences(processed_text, misspelled_indices)\n",
        "    words = processed_text.split()\n",
        "    corrections = {}\n",
        "\n",
        "    for idx, masked in zip(misspelled_indices, masked_sentences):\n",
        "        original_word = words[idx]\n",
        "        candidates = predict(masked)\n",
        "        if candidates:\n",
        "            best_candidate = min(candidates, key=lambda c: levenshtein_distance(c, original_word))\n",
        "            corrections[original_word] = best_candidate\n",
        "            words[idx] = best_candidate\n",
        "\n",
        "    corrected_sentence = \" \".join(words)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"ğŸ” Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØµØ­ÙŠØ­Ù‡Ø§:\")\n",
        "        for original, corrected in corrections.items():\n",
        "            print(f\" - {original} â¤ {corrected}\")\n",
        "\n",
        "    return corrected_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_zlKGTc9B_n",
        "outputId": "0dfe7226-aea9-434d-81dd-ce394fe5bfba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                text  target\n",
            "0  Ø¨ÙŠÙ† Ø£Ø³ØªÙˆØ¯ÙŠÙˆÙ‡Ø§Øª ÙˆØ±Ø²Ø§Ø²Ø§Øª ÙˆØµØ­Ø±Ø§Ø¡ Ù…Ø±Ø²ÙˆÙƒØ© ÙˆØ¢Ø«Ø§Ø± ÙˆÙ„ÙŠ...       0\n",
            "1  Ù‚Ø±Ø±Øª Ø§Ù„Ù†Ø¬Ù…Ø© Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠØ© Ø£ÙˆØ¨Ø±Ø§ ÙˆÙŠÙ†ÙØ±ÙŠ Ø£Ù„Ø§ ÙŠÙ‚ØªØµØ± Ø¹...       0\n",
            "2  Ø£Ø®Ø¨Ø§Ø±Ù†Ø§ Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© Ø§Ù„ÙˆØ²Ø§Ù†ÙŠ ØªØµÙˆÙŠØ± Ø§Ù„Ø´Ù…Ù„Ø§Ù„ÙŠ Ø£Ù„Ù‡Ø¨ Ø§...       0\n",
            "3  Ø§Ø®Ø¨Ø§Ø±Ù†Ø§ Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ© Ù‚Ø§Ù„ Ø§Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù„Ø±Ø§Ø´Ø¯ÙŠ Ù…Ø­Ø§Ù…ÙŠ Ø³Ø¹Ø¯...       0\n",
            "4  ØªØ²Ø§Ù„ ØµÙ†Ø§Ø¹Ø© Ø§Ù„Ø¬Ù„ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…ØºØ±Ø¨ ØªØªØ¨Ø¹ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙ‚Ù„...       0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the .txt file as if it's a CSV\n",
        "df = pd.read_csv('/content/drive/MyDrive/arabic_dataset_classifiction.txt', encoding='utf-8')\n",
        "\n",
        "# Optional: Fix column name if needed\n",
        "df.columns = ['text', 'target']\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkCrU4NwoTSC"
      },
      "outputs": [],
      "source": [
        "# Clean dataset\n",
        "df = df.drop(columns=['targe'], errors='ignore').dropna().drop_duplicates()\n",
        "df['text'] = df['text'].apply(preprocess)\n",
        "df['text'] = df['text'].apply(lambda x: x if len(x.split()) > 5 else None)\n",
        "df = df.dropna().reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8gOxOLG9K_m"
      },
      "outputs": [],
      "source": [
        "# Create vocabulary\n",
        "words_freq = data_vocab(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a6685aebe8204834b878d145828e5c3f",
            "2fbd04f25696401db5c16c3c4621759a",
            "531d266653b245ec9988ea19eeb393f8",
            "2bc15f7210c141aabfd495a6760b07a7",
            "66b872f4f2bb4960b48f1b58abceb94d",
            "3c6b179b2eca43d78acc2d67633d8b3a",
            "18727913e7234c73985d864f0c61800a",
            "54a0d3e6758b48a7b7c17b24603bc47d",
            "8e9bdc49b57a4d97b52428b7f201ff3b",
            "9063ffa00fdb4235adfeb31faee865ac",
            "6c828f340c494cd78a9e83b0ab813288"
          ]
        },
        "id": "hkkI6xeq9MnN",
        "outputId": "adf52fd6-8e42-4704-df3b-63fac69cdc68"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6685aebe8204834b878d145828e5c3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Prepare Hugging Face dataset\n",
        "dataset = Dataset.from_pandas(df[:10000])  # Use smaller subset for training\n",
        "dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "dataset = dataset.train_test_split(test_size=0.2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgjqpGnqshgK"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./model\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    gradient_accumulation_steps=2,\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    optim=\"adamw_torch\"\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtrH1Um59Ttt",
        "outputId": "16451a95-df15-4f3d-ed92-7ce3fe90a00d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-328daa5726ac>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "# Optional: Use a smaller subset for faster testing\n",
        "train_subset = dataset[\"train\"].select(range(1000))\n",
        "test_subset = dataset[\"test\"].select(range(200))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_subset,\n",
        "    eval_dataset=test_subset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "PoBzBDr0pF6x",
        "outputId": "2ab8b0e3-8b64-4543-b0d8-13553a7e866e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myahiahanii45\u001b[0m (\u001b[33myahiahanii45-helwan-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250513_031735-65yxfajj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/yahiahanii45-helwan-university/huggingface/runs/65yxfajj' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/yahiahanii45-helwan-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/yahiahanii45-helwan-university/huggingface' target=\"_blank\">https://wandb.ai/yahiahanii45-helwan-university/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/yahiahanii45-helwan-university/huggingface/runs/65yxfajj' target=\"_blank\">https://wandb.ai/yahiahanii45-helwan-university/huggingface/runs/65yxfajj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train and save model\n",
        "trainer.train()\n",
        "trainer.save_model(\"/content/my_model\")\n",
        "tokenizer.save_pretrained(\"/content/my_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bVSNodCA1oP",
        "outputId": "703ba325-9fbd-479b-ce74-bfecb6007b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØµØ­ÙŠØ­Ù‡Ø§:\n",
            " - ÙˆØ²Ø²Ø§Ø±Ø© â¤ ÙˆØ²Ø§Ø±Ø©\n",
            " - Ø§Ù„Ù†Ø±Ø¨ÙŠØ© â¤ Ø§Ù„ØªØ±Ø¨ÙŠØ©\n",
            " - ØªØ¹Ø·Ø¨Ù„ â¤ ØªØ¹Ø·ÙŠÙ„\n",
            " - Ø§Ù„Ø³Ø³Ø¨Øª â¤ Ø§Ù„Ø³Ø¨Øª\n",
            " - Ø§Ù„Ø­Ø§Ù„ÙŠÙŠÙ‡ â¤ Ø§Ù„Ø­Ø§Ù„ÙŠÙ‡\n",
            " - Ø§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† â¤ Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ†\n",
            " - Ø§Ù„Ø·Ø·Ù„Ø§Ø¨ â¤ Ø§Ù„Ø·Ù„Ø§Ø¨\n",
            "Incorrect Sentence: ÙˆØ²Ø²Ø§Ø±Ø© Ø§Ù„Ù†Ø±Ø¨ÙŠØ© ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ… ØªØ¹Ù„Ù† Ø¹Ù† ØªØ¹Ø·Ø¨Ù„ Ø§Ù„Ø¯Ø±Ø§Ø³Ø© Ø±Ø³Ù…ÙŠØ§ ÙŠÙˆÙ… Ø§Ù„Ø³Ø³Ø¨Øª Ù†Ø¸Ø±Ø§ Ù„Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ø¬ÙˆÙŠØ© Ø§Ù„Ø­Ø§Ù„ÙŠÙŠÙ‡ ÙˆØ­ÙØ§Ø¸Ø§ Ø¹Ù„Ù‰ Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† Ùˆ Ø§Ù„Ø·Ø·Ù„Ø§Ø¨\n",
            "Corrected Sentence: ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ±Ø¨ÙŠØ© ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ… ØªØ¹Ù„Ù† Ø¹Ù† ØªØ¹Ø·ÙŠÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³Ø© Ø±Ø³Ù…ÙŠØ§ ÙŠÙˆÙ… Ø§Ù„Ø³Ø¨Øª Ù†Ø¸Ø±Ø§ Ù„Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ø¬ÙˆÙŠØ© Ø§Ù„Ø­Ø§Ù„ÙŠÙ‡ ÙˆØ­ÙØ§Ø¸Ø§ Ø¹Ù„Ù‰ Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ù…Ø¹Ù„Ù…ÙŠÙ† Ùˆ Ø§Ù„Ø·Ù„Ø§Ø¨\n",
            "--------------------\n",
            "ğŸ” Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØµØ­ÙŠØ­Ù‡Ø§:\n",
            " - Ø§Ù„ÙÙ†Ø³ÙŠ â¤ Ø§Ù„ÙØ±Ù†Ø³ÙŠ\n",
            " - Ø§Ù„Ø­Ù…ÙŠØ³ â¤ Ø§Ù„Ø®Ù…ÙŠØ³\n",
            " - Ø§Ù„Ù‚ØªØªÙ„ â¤ Ø§Ù„Ù‚ØªÙ„\n",
            "Incorrect Sentence: Ø¨Ø¹Ø¯ Ø£Ù† Ù‚Ø±Ø± Ø§Ù„Ù‚Ø¶Ø§Ø¡ Ø§Ù„ÙÙ†Ø³ÙŠ Ø§Ù„Ø¥Ø¨Ù‚Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØºÙ†ÙŠ Ø§Ù„Ù…ØºØ±Ø¨ÙŠ Ø³Ø¹Ø¯ Ù„Ù…Ø¬Ø±Ø¯ Ø§Ù„Ø°ÙŠ Ù‚Ø¨Ø¶ Ø¹Ù„ÙŠÙ‡ ÙŠÙˆÙ… Ø§Ù„Ø­Ù…ÙŠØ³ Ø¨ØªÙ‡Ù…ØªÙŠ Ø§Ù„Ù‚ØªØªÙ„ Ùˆ Ø§Ù„ØªØ­Ø±Ø´\n",
            "Corrected Sentence: Ø¨Ø¹Ø¯ Ø§Ù† Ù‚Ø±Ø± Ø§Ù„Ù‚Ø¶Ø§Ø¡ Ø§Ù„ÙØ±Ù†Ø³ÙŠ Ø§Ù„Ø§Ø¨Ù‚Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØºÙ†ÙŠ Ø§Ù„Ù…ØºØ±Ø¨ÙŠ Ø³Ø¹Ø¯ Ù„Ù…Ø¬Ø±Ø¯ Ø§Ù„Ø°ÙŠ Ù‚Ø¨Ø¶ Ø¹Ù„ÙŠÙ‡ ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³ Ø¨ØªÙ‡Ù…ØªÙŠ Ø§Ù„Ù‚ØªÙ„ Ùˆ Ø§Ù„ØªØ­Ø±Ø´\n",
            "--------------------\n",
            "ğŸ” Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØµØ­ÙŠØ­Ù‡Ø§:\n",
            " - Ø§Ù„Ø§Ø³Ù„Ø§Ù…ÙŠÙ‡ â¤ Ø§Ù„Ø§Ø³Ù„Ø§Ù…ÙŠÙ‡\n",
            " - Ø­Ø±Ø¨Ø¨ â¤ Ø®Ø±Ø§Ø¨\n",
            "Incorrect Sentence: Ù‚Ø§Ù„ Ø§Ù„ÙÙ†Ø§Ù† Ø§Ù„Ù…ØºØ±Ø¨ÙŠ Ù…Ø­Ù…Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±ÙŠ Ù†Ø­Ù† Ø§Ø­Ø±Ø§Ø± Ø§Ù„Ø§Ù…Ù‡ Ø§Ù„Ø§Ø³Ù„Ø§Ù…ÙŠÙ‡ Ùˆ Ù„Ù† Ù†Ø³Ù…Ø­ Ø¨Ø­Ø¯ÙˆØ« Ø­Ø±Ø¨Ø¨ ÙÙŠ Ø§Ù„ÙˆØ·Ù†\n",
            "Corrected Sentence: Ù‚Ø§Ù„ Ø§Ù„ÙÙ†Ø§Ù† Ø§Ù„Ù…ØºØ±Ø¨ÙŠ Ù…Ø­Ù…Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±ÙŠ Ù†Ø­Ù† Ø§Ø­Ø±Ø§Ø± Ø§Ù„Ø§Ù…Ù‡ Ø§Ù„Ø§Ø³Ù„Ø§Ù…ÙŠÙ‡ Ùˆ Ù„Ù† Ù†Ø³Ù…Ø­ Ø¨Ø­Ø¯ÙˆØ« Ø®Ø±Ø§Ø¨ ÙÙŠ Ø§Ù„ÙˆØ·Ù†\n",
            "--------------------\n",
            "ğŸ” Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØµØ­ÙŠØ­Ù‡Ø§:\n",
            " - ÙŠØ°Ù‡Ù‡Ø¨ÙˆÙ† â¤ ÙŠØ°Ù‡Ø¨ÙˆÙ†\n",
            " - Ø§Ù„Ù…Ø¯Ø±Ø³Ù‡ â¤ Ø§Ù„Ù…Ø¯Ø±Ø³Ù‡\n",
            " - Ø§Ù„Ø¨Ø§ÙƒÙƒØ± â¤ Ø§Ù„Ø¨Ø§ÙƒØ±\n",
            "Incorrect Sentence: Ø§Ù„Ø·Ù„Ø§Ø¨ ÙŠØ°Ù‡Ù‡Ø¨ÙˆÙ† Ø§Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ù‡ ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­ Ø§Ù„Ø¨Ø§ÙƒÙƒØ± Ù„ØªÙ„Ù‚ÙŠ Ø§Ù„Ø¯Ø±ÙˆØ³\n",
            "Corrected Sentence: Ø§Ù„Ø·Ù„Ø§Ø¨ ÙŠØ°Ù‡Ø¨ÙˆÙ† Ø§Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ù‡ ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­ Ø§Ù„Ø¨Ø§ÙƒØ± Ù„ØªÙ„Ù‚ÙŠ Ø§Ù„Ø¯Ø±ÙˆØ³\n",
            "--------------------\n",
            "ğŸ” Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØµØ­ÙŠØ­Ù‡Ø§:\n",
            " - Ø§Ù„Ù‚Ø§Ù‡Ø± â¤ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©\n",
            "Incorrect Sentence: ÙˆØµÙ„ Ø§Ù„Ø±ÙŠØ³ Ø§Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù‡Ø± ØµØ¨Ø§Ø­ Ø§Ù„ÙŠÙˆÙ… Ù„Ø¹Ù‚Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ù‡Ù… Ù…Ø¹ Ø§Ù„ÙˆØ²Ø±Ø§Ø¡\n",
            "Corrected Sentence: ÙˆØµÙ„ Ø§Ù„Ø±ÙŠØ³ Ø§Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© ØµØ¨Ø§Ø­ Ø§Ù„ÙŠÙˆÙ… Ù„Ø¹Ù‚Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ù‡Ù… Ù…Ø¹ Ø§Ù„ÙˆØ²Ø±Ø§Ø¡\n",
            "--------------------\n",
            "ğŸ” Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØµØ­ÙŠØ­Ù‡Ø§:\n",
            " - Ø§Ù„Ù…Ø§ÙŠÙ‡ â¤ Ø§Ù„Ù…ÙŠØ§Ù‡\n",
            "Incorrect Sentence: ÙŠØ¹Ø§Ù†ÙŠ Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ÙˆÙ† Ù…Ù† Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ ØªÙˆÙØ± Ø§Ù„Ù…Ø§ÙŠÙ‡ Ù„Ù„Ø±ÙŠ ÙÙŠ ÙØµÙ„ Ø§Ù„ØµÙŠÙ\n",
            "Corrected Sentence: ÙŠØ¹Ø§Ù†ÙŠ Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ÙˆÙ† Ù…Ù† Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ ØªÙˆÙØ± Ø§Ù„Ù…ÙŠØ§Ù‡ Ù„Ù„Ø±ÙŠ ÙÙŠ ÙØµÙ„ Ø§Ù„ØµÙŠÙ\n",
            "--------------------\n",
            "ğŸ” Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØµØ­ÙŠØ­Ù‡Ø§:\n",
            " - Ù…Ø¯Ø§ÙˆÙ„Ù‡ â¤ Ù…Ù†Ø§ÙØ³Ù‡\n",
            " - Ø·ÙˆÙŠÙ„Ù‡ â¤ Ø¬Ù„Ø³Ù‡\n",
            " - Ø§Ù„Ù…Ø­ÙƒÙ…Ù‡ â¤ Ø§Ù„Ù…Ø­ÙƒÙ…Ù‡\n",
            "Incorrect Sentence: Ø§ØµØ¯Ø± Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ø­ÙƒÙ…Ù‡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ø¹Ø¯ Ù…Ø¯Ø§ÙˆÙ„Ù‡ Ø·ÙˆÙŠÙ„Ù‡ Ø¨ÙŠÙ† Ø§Ø¹Ø¶Ø§Ø¡ Ø§Ù„Ù…Ø­ÙƒÙ…Ù‡\n",
            "Corrected Sentence: Ø§ØµØ¯Ø± Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ø­ÙƒÙ…Ù‡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ø¹Ø¯ Ù…Ù†Ø§ÙØ³Ù‡ Ø¬Ù„Ø³Ù‡ Ø¨ÙŠÙ† Ø§Ø¹Ø¶Ø§Ø¡ Ø§Ù„Ù…Ø­ÙƒÙ…Ù‡\n",
            "--------------------\n",
            "ğŸ” Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØµØ­ÙŠØ­Ù‡Ø§:\n",
            " - Ù…Ù‡Ù…Ù‡ â¤ Ù…Ù‡Ù…Ù‡\n",
            " - Ø§Ù„Ù…Ø¤Ù‡Ù„Ù‡ â¤ Ø§Ù„Ù…ÙˆÙ‡Ù„Ø©\n",
            " - Ø§Ù„Ø¹Ø§Ù„Ù„Ù… â¤ Ø§Ù„Ø¹Ø§Ù„Ù…\n",
            "Incorrect Sentence: Ø§Ù„Ù…Ù†ØªØ®Ø¨ Ø§Ù„ÙˆØ·Ù†ÙŠ ÙŠØ®Ø¶ Ù…Ø¨Ø§Ø±Ø§Ø© Ù…Ù‡Ù…Ù‡ ÙÙŠ Ø§Ù„ØªØµÙÙŠØ§Øª Ø§Ù„Ù…Ø¤Ù‡Ù„Ù‡ Ù„ÙƒØ£Ø³ Ø§Ù„Ø¹Ø§Ù„Ù„Ù…\n",
            "Corrected Sentence: Ø§Ù„Ù…Ù†ØªØ®Ø¨ Ø§Ù„ÙˆØ·Ù†ÙŠ ÙŠØ®Ø¶ Ù…Ø¨Ø§Ø±Ø§Ø© Ù…Ù‡Ù…Ù‡ ÙÙŠ Ø§Ù„ØªØµÙÙŠØ§Øª Ø§Ù„Ù…ÙˆÙ‡Ù„Ø© Ù„ÙƒØ§Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "input_texts = [\n",
        "    \"ÙˆØ²Ø²Ø§Ø±Ø© Ø§Ù„Ù†Ø±Ø¨ÙŠØ© ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ… ØªØ¹Ù„Ù† Ø¹Ù† ØªØ¹Ø·Ø¨Ù„ Ø§Ù„Ø¯Ø±Ø§Ø³Ø© Ø±Ø³Ù…ÙŠØ§ ÙŠÙˆÙ… Ø§Ù„Ø³Ø³Ø¨Øª Ù†Ø¸Ø±Ø§ Ù„Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ø¬ÙˆÙŠØ© Ø§Ù„Ø­Ø§Ù„ÙŠÙŠÙ‡ ÙˆØ­ÙØ§Ø¸Ø§ Ø¹Ù„Ù‰ Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† Ùˆ Ø§Ù„Ø·Ø·Ù„Ø§Ø¨\",\n",
        "    \"Ø¨Ø¹Ø¯ Ø£Ù† Ù‚Ø±Ø± Ø§Ù„Ù‚Ø¶Ø§Ø¡ Ø§Ù„ÙÙ†Ø³ÙŠ Ø§Ù„Ø¥Ø¨Ù‚Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØºÙ†ÙŠ Ø§Ù„Ù…ØºØ±Ø¨ÙŠ Ø³Ø¹Ø¯ Ù„Ù…Ø¬Ø±Ø¯ Ø§Ù„Ø°ÙŠ Ù‚Ø¨Ø¶ Ø¹Ù„ÙŠÙ‡ ÙŠÙˆÙ… Ø§Ù„Ø­Ù…ÙŠØ³ Ø¨ØªÙ‡Ù…ØªÙŠ Ø§Ù„Ù‚ØªØªÙ„ Ùˆ Ø§Ù„ØªØ­Ø±Ø´\",\n",
        "    \"Ù‚Ø§Ù„ Ø§Ù„ÙÙ†Ø§Ù† Ø§Ù„Ù…ØºØ±Ø¨ÙŠ Ù…Ø­Ù…Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±ÙŠ Ù†Ø­Ù† Ø§Ø­Ø±Ø§Ø± Ø§Ù„Ø§Ù…Ù‡ Ø§Ù„Ø§Ø³Ù„Ø§Ù…ÙŠÙ‡ Ùˆ Ù„Ù† Ù†Ø³Ù…Ø­ Ø¨Ø­Ø¯ÙˆØ« Ø­Ø±Ø¨Ø¨ ÙÙŠ Ø§Ù„ÙˆØ·Ù†\",\n",
        "    \"Ø§Ù„Ø·Ù„Ø§Ø¨ ÙŠØ°Ù‡Ù‡Ø¨ÙˆÙ† Ø§Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ù‡ ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­ Ø§Ù„Ø¨Ø§ÙƒÙƒØ± Ù„ØªÙ„Ù‚ÙŠ Ø§Ù„Ø¯Ø±ÙˆØ³\",\n",
        "    \"ÙˆØµÙ„ Ø§Ù„Ø±ÙŠØ³ Ø§Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù‡Ø± ØµØ¨Ø§Ø­ Ø§Ù„ÙŠÙˆÙ… Ù„Ø¹Ù‚Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ù‡Ù… Ù…Ø¹ Ø§Ù„ÙˆØ²Ø±Ø§Ø¡\",\n",
        "    \"ÙŠØ¹Ø§Ù†ÙŠ Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ÙˆÙ† Ù…Ù† Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ ØªÙˆÙØ± Ø§Ù„Ù…Ø§ÙŠÙ‡ Ù„Ù„Ø±ÙŠ ÙÙŠ ÙØµÙ„ Ø§Ù„ØµÙŠÙ\",\n",
        "    \"Ø§ØµØ¯Ø± Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ø­ÙƒÙ…Ù‡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ø¹Ø¯ Ù…Ø¯Ø§ÙˆÙ„Ù‡ Ø·ÙˆÙŠÙ„Ù‡ Ø¨ÙŠÙ† Ø§Ø¹Ø¶Ø§Ø¡ Ø§Ù„Ù…Ø­ÙƒÙ…Ù‡\",\n",
        "    \"Ø§Ù„Ù…Ù†ØªØ®Ø¨ Ø§Ù„ÙˆØ·Ù†ÙŠ ÙŠØ®Ø¶ Ù…Ø¨Ø§Ø±Ø§Ø© Ù…Ù‡Ù…Ù‡ ÙÙŠ Ø§Ù„ØªØµÙÙŠØ§Øª Ø§Ù„Ù…Ø¤Ù‡Ù„Ù‡ Ù„ÙƒØ£Ø³ Ø§Ù„Ø¹Ø§Ù„Ù„Ù…\",\n",
        "]\n",
        "\n",
        "for input_text in input_texts:\n",
        "    # Pass words_freq as the vocab argument\n",
        "    true_sentence = pipeline(input_text, words_freq)\n",
        "    print('Incorrect Sentence:', input_text)\n",
        "    print('Corrected Sentence:', true_sentence)\n",
        "    print('-' * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIPiSYZQOMm5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define test dataset with ground-truth corrections\n",
        "test_df = pd.DataFrame({\n",
        "    'incorrect': input_texts,\n",
        "    'correct': [\n",
        "        \"ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØªØ±Ø¨ÙŠØ© ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ… ØªØ¹Ù„Ù† Ø¹Ù† ØªØ¹Ø·ÙŠÙ„ Ø§Ù„Ø¯Ø±Ø§Ø³Ø© Ø±Ø³Ù…ÙŠØ§ ÙŠÙˆÙ… Ø§Ù„Ø³Ø¨Øª Ù†Ø¸Ø±Ø§ Ù„Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ø¬ÙˆÙŠØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ© ÙˆØ­ÙØ§Ø¸Ø§ Ø¹Ù„Ù‰ Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ù…Ø¹Ù„Ù…ÙˆÙ† Ùˆ Ø§Ù„Ø·Ù„Ø§Ø¨\",\n",
        "        \"Ø¨Ø¹Ø¯ Ø§Ù† Ù‚Ø±Ø± Ø§Ù„Ù‚Ø¶Ø§Ø¡ Ø§Ù„ÙØ±Ù†Ø³ÙŠ Ø§Ù„Ø§Ø¨Ù‚Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØºÙ†ÙŠ Ø§Ù„Ù…ØºØ±Ø¨ÙŠ Ø³Ø¹Ø¯ Ù„Ù…Ø¬Ø±Ø¯ Ø§Ù„Ø°ÙŠ Ù‚Ø¨Ø¶ Ø¹Ù„ÙŠÙ‡ ÙŠÙˆÙ… Ø§Ù„Ø®Ù…ÙŠØ³ Ø¨ØªÙ‡Ù…ØªÙŠ Ø§Ù„Ù‚ØªÙ„ Ùˆ Ø§Ù„ØªØ­Ø±Ø´\",\n",
        "        \"Ù‚Ø§Ù„ Ø§Ù„ÙÙ†Ø§Ù† Ø§Ù„Ù…ØºØ±Ø¨ÙŠ Ù…Ø­Ù…Ø¯ Ø§Ù„Ø®ÙŠØ§Ø±ÙŠ Ù†Ø­Ù† Ø§Ø­Ø±Ø§Ø± Ø§Ù„Ø§Ù…Ù‡ Ø§Ù„Ø§Ø³Ù„Ø§Ù…ÙŠØ© Ùˆ Ù„Ù† Ù†Ø³Ù…Ø­ Ø¨Ø­Ø¯ÙˆØ« Ø­Ø±Ø¨ ÙÙŠ Ø§Ù„ÙˆØ·Ù†\",\n",
        "        \"Ø§Ù„Ø·Ù„Ø§Ø¨ ÙŠØ°Ù‡Ø¨ÙˆÙ† Ø§Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø© ÙÙŠ Ø§Ù„ØµØ¨Ø§Ø­ Ø§Ù„Ø¨Ø§ÙƒØ± Ù„ØªÙ„Ù‚ÙŠ Ø§Ù„Ø¯Ø±ÙˆØ³\",\n",
        "        \"ÙˆØµÙ„ Ø§Ù„Ø±ÙŠØ³ Ø§Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© ØµØ¨Ø§Ø­ Ø§Ù„ÙŠÙˆÙ… Ù„Ø¹Ù‚Ø¯ Ø§Ø¬ØªÙ…Ø§Ø¹ Ù…Ù‡Ù… Ù…Ø¹ Ø§Ù„ÙˆØ²Ø±Ø§Ø¡\",\n",
        "        \"ÙŠØ¹Ø§Ù†ÙŠ Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ÙˆÙ† Ù…Ù† Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ ØªÙˆÙØ± Ø§Ù„Ù…Ø§Ø¡ Ù„Ù„Ø±ÙŠ ÙÙŠ ÙØµÙ„ Ø§Ù„ØµÙŠÙ\",\n",
        "        \"Ø§ØµØ¯Ø± Ø§Ù„Ù‚Ø§Ø¶ÙŠ Ø­ÙƒÙ…Ù‡ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ø¹Ø¯ Ù…Ø¯Ø§ÙˆÙ„Ø© Ø·ÙˆÙŠÙ„Ø© Ø¨ÙŠÙ† Ø§Ø¹Ø¶Ø§Ø¡ Ø§Ù„Ù…Ø­ÙƒÙ…Ø©\",\n",
        "        \"Ø§Ù„Ù…Ù†ØªØ®Ø¨ Ø§Ù„ÙˆØ·Ù†ÙŠ ÙŠØ®ÙˆØ¶ Ù…Ø¨Ø§Ø±Ø§Ø© Ù…Ù‡Ù…Ø© ÙÙŠ Ø§Ù„ØªØµÙÙŠØ§Øª Ø§Ù„Ù…Ø¤Ù‡Ù„Ø© Ù„ÙƒØ§Ø³ Ø§Ù„Ø¹Ø§Ù„Ù…\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "def evaluate_pipeline(test_df, pipeline_func, df):\n",
        "    y_true_words = []\n",
        "    y_pred_words = []\n",
        "    sentence_correct = []\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        incorrect = row['incorrect']\n",
        "        true_sentence = row['correct']\n",
        "\n",
        "        # Run pipeline\n",
        "        pred_sentence = pipeline_func(incorrect, df, verbose=False)\n",
        "\n",
        "        # Split sentences into words\n",
        "        true_words = true_sentence.split()\n",
        "        pred_words = pred_sentence.split()\n",
        "        incorrect_words = incorrect.split()\n",
        "\n",
        "        # Ensure same length for comparison\n",
        "        min_len = min(len(true_words), len(pred_words), len(incorrect_words))\n",
        "        true_words = true_words[:min_len]\n",
        "        pred_words = pred_words[:min_len]\n",
        "        incorrect_words = incorrect_words[:min_len]\n",
        "\n",
        "        # Word-level comparison for misspelled words\n",
        "        for i in range(min_len):\n",
        "            if incorrect_words[i] != true_words[i]:  # Misspelled word\n",
        "                y_true_words.append(true_words[i])\n",
        "                y_pred_words.append(pred_words[i])\n",
        "\n",
        "        # Sentence-level accuracy\n",
        "        sentence_correct.append(true_sentence == pred_sentence)\n",
        "\n",
        "    # Calculate metrics\n",
        "    word_accuracy = accuracy_score(y_true_words, y_pred_words)\n",
        "    word_f1 = f1_score(y_true_words, y_pred_words, average='weighted', zero_division=0)\n",
        "    conf_matrix = confusion_matrix(y_true_words, y_pred_words)\n",
        "    sentence_accuracy = np.mean(sentence_correct)\n",
        "\n",
        "    return {\n",
        "        'word_accuracy': word_accuracy,\n",
        "        'word_f1': word_f1,\n",
        "        'confusion_matrix': conf_matrix,\n",
        "        'sentence_accuracy': sentence_accuracy,\n",
        "        'y_true_words': y_true_words,\n",
        "        'y_pred_words': y_pred_words\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrIwkkwFOljL",
        "outputId": "1fca0a14-0d47-4f4a-cf3a-dfb384ba59ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluation Section ===\n",
            "Evaluation Results:\n",
            "Word-Level Accuracy: 0.5769\n",
            "Word-Level F1 Score: 0.5769\n",
            "Sentence-Level Accuracy: 0.2500\n",
            "Confusion Matrix (Word-Level):\n",
            "[[1 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 1]]\n",
            "\n",
            "Detailed Errors:\n",
            "True: Ø§Ù„Ø­Ø§Ù„ÙŠØ©, Predicted: Ø§Ù„Ø­Ø§Ù„ÙŠÙ‡\n",
            "True: Ø§Ù„Ø§Ø³Ù„Ø§Ù…ÙŠØ©, Predicted: Ø§Ù„Ø§Ø³Ù„Ø§Ù…ÙŠÙ‡\n",
            "True: Ø­Ø±Ø¨, Predicted: Ø®Ø±Ø§Ø¨\n",
            "True: Ø§Ù„Ù…Ø¯Ø±Ø³Ø©, Predicted: Ø§Ù„Ù…Ø¯Ø±Ø³Ù‡\n",
            "True: Ø§Ù„Ù…Ø§Ø¡, Predicted: Ø§Ù„Ù…ÙŠØ§Ù‡\n",
            "True: Ù…Ø¯Ø§ÙˆÙ„Ø©, Predicted: Ù…Ù†Ø§ÙØ³Ù‡\n",
            "True: Ø·ÙˆÙŠÙ„Ø©, Predicted: Ø¬Ù„Ø³Ù‡\n",
            "True: Ø§Ù„Ù…Ø­ÙƒÙ…Ø©, Predicted: Ø§Ù„Ù…Ø­ÙƒÙ…Ù‡\n",
            "True: ÙŠØ®ÙˆØ¶, Predicted: ÙŠØ®Ø¶\n",
            "True: Ù…Ù‡Ù…Ø©, Predicted: Ù…Ù‡Ù…Ù‡\n",
            "True: Ø§Ù„Ù…Ø¤Ù‡Ù„Ø©, Predicted: Ø§Ù„Ù…ÙˆÙ‡Ù„Ø©\n"
          ]
        }
      ],
      "source": [
        "# Evaluation Section\n",
        "print(\"\\n=== Evaluation Section ===\")\n",
        "\n",
        "# Run evaluation\n",
        "results = evaluate_pipeline(test_df, pipeline, df)\n",
        "\n",
        "# Print evaluation results\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Word-Level Accuracy: {results['word_accuracy']:.4f}\")\n",
        "print(f\"Word-Level F1 Score: {results['word_f1']:.4f}\")\n",
        "print(f\"Sentence-Level Accuracy: {results['sentence_accuracy']:.4f}\")\n",
        "print(\"Confusion Matrix (Word-Level):\")\n",
        "print(results['confusion_matrix'])\n",
        "print(\"\\nDetailed Errors:\")\n",
        "for true, pred in zip(results['y_true_words'], results['y_pred_words']):\n",
        "    if true != pred:\n",
        "        print(f\"True: {true}, Predicted: {pred}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "WXTxJoTv4eAi",
        "outputId": "57f07953-5ac9-48a9-90a3-937a456657bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/my_model/ (stored 0%)\n",
            "  adding: content/my_model/special_tokens_map.json (deflated 14%)\n",
            "  adding: content/my_model/vocab.txt (deflated 61%)\n",
            "  adding: content/my_model/tokenizer_config.json (deflated 14%)\n",
            "  adding: content/my_model/model.safetensors (deflated 14%)\n",
            "  adding: content/my_model/training_args.bin (deflated 15%)\n",
            "  adding: content/my_model/config.json (deflated 13%)\n",
            "  adding: content/my_model/tokenizer.json (deflated 14%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_44a4f6cf-25e1-4143-8cbf-068269838346\", \"my_model.zip\", 428221)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "!zip -r my_model.zip /content/my_model\n",
        "files.download('my_model.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWGqj9GDXUyA",
        "outputId": "31832ecf-71c9-4a99-a9ac-0985d294c4ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.29.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.11/dist-packages (0.27.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: Levenshtein==0.27.1 in /usr/local/lib/python3.11/dist-packages (from python-Levenshtein) (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio transformers torch pandas python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "sLW2jS_sXZg8",
        "outputId": "0335afbf-f177-4c18-e6c2-cb7eb372d357"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://10a41491113f6caf97.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://10a41491113f6caf97.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "# ==== Load model ====\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = \"/content/my_model\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"asafaya/bert-base-arabic\").to(device)\n",
        "model.eval()\n",
        "\n",
        "# Read the .txt file as if it's a CSV\n",
        "df = pd.read_csv('/content/drive/MyDrive/arabic_dataset_classifiction.txt', encoding='utf-8')\n",
        "\n",
        "# Optional: Fix column name if needed\n",
        "df.columns = ['text', 'target']\n",
        "\n",
        "\n",
        "def preprocess(sentence: str) -> str:\n",
        "    sentence = sentence.replace(\"Ø£\", \"Ø§\").replace(\"Ø¥\", \"Ø§\").replace(\"Ø¢\", \"Ø§\")\n",
        "    sentence = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \"\", sentence)\n",
        "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n",
        "    return sentence\n",
        "\n",
        "def data_vocab(dataframe, min_freq=3):\n",
        "    words_freq = Counter()\n",
        "    for text in dataframe[\"text\"]:\n",
        "        words_freq.update(text.split())\n",
        "    return {word: freq for word, freq in words_freq.items() if freq >= min_freq}\n",
        "\n",
        "def normalize_hamza(word: str) -> str:\n",
        "    return (\n",
        "        word.replace(\"Ø£\", \"Ø§\")\n",
        "        .replace(\"Ø¥\", \"Ø§\")\n",
        "        .replace(\"Ø¤\", \"Ùˆ\")\n",
        "        .replace(\"Ø¦\", \"ÙŠ\")\n",
        "        .replace(\"Ø¡\", \"\")\n",
        "    )\n",
        "\n",
        "def find_misspellings(text: str, vocab: dict, threshold: float = 0.28) -> list:\n",
        "    words = text.split()\n",
        "    misspelled_indices = []\n",
        "    for i, word in enumerate(words):\n",
        "        if word not in vocab and normalize_hamza(word) not in vocab:\n",
        "            masked_words = words.copy()\n",
        "            masked_words[i] = tokenizer.mask_token\n",
        "            masked_sentence = \" \".join(masked_words)\n",
        "            inputs = tokenizer(masked_sentence, return_tensors=\"pt\").to(device)\n",
        "            mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits[0, mask_token_index]\n",
        "                probs = torch.softmax(logits, dim=-1).squeeze()\n",
        "                word_id = tokenizer.encode(word, add_special_tokens=False)\n",
        "                word_prob = torch.mean(probs[word_id]) if word_id else 0\n",
        "            if word_prob < threshold:\n",
        "                misspelled_indices.append(i)\n",
        "    return misspelled_indices\n",
        "\n",
        "def generate_masked_sentences(text: str, misspelled_indices: list) -> list:\n",
        "    words = text.split()\n",
        "    return [\n",
        "        \" \".join(words[:idx] + [tokenizer.mask_token] + words[idx + 1:])\n",
        "        for idx in misspelled_indices\n",
        "    ]\n",
        "\n",
        "def predict(masked_sentence: str, top_k=25) -> list:\n",
        "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\").to(device)\n",
        "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits[0, mask_token_index]\n",
        "    probs = torch.softmax(logits, dim=-1).squeeze()\n",
        "    top_k_tokens = torch.topk(probs, top_k)\n",
        "    predictions = []\n",
        "    for token_id in top_k_tokens.indices:\n",
        "        token = tokenizer.decode([token_id]).strip()\n",
        "        if re.match(r\"^[\\u0600-\\u06FF]{2,}$\", token):\n",
        "            predictions.append(token)\n",
        "    return predictions\n",
        "\n",
        "def pipeline(input_text: str, vocab: dict) -> str:\n",
        "    processed_text = preprocess(input_text)\n",
        "    misspelled_indices = find_misspellings(processed_text, vocab)\n",
        "\n",
        "    if not misspelled_indices:\n",
        "        return f\"âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø®Ø·Ø§Ø¡ Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©:\\n\\n{processed_text}\"\n",
        "\n",
        "    masked_sentences = generate_masked_sentences(processed_text, misspelled_indices)\n",
        "    words = processed_text.split()\n",
        "    corrections = {}\n",
        "\n",
        "    for idx, masked in zip(misspelled_indices, masked_sentences):\n",
        "        original_word = words[idx]\n",
        "        candidates = predict(masked)\n",
        "        if candidates:\n",
        "            best_candidate = min(candidates, key=lambda c: levenshtein_distance(c, original_word))\n",
        "            corrections[original_word] = best_candidate\n",
        "            words[idx] = best_candidate\n",
        "\n",
        "    corrected_sentence = \" \".join(words)\n",
        "    corrections_text = \"ğŸ” Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªØµØ­ÙŠØ­Ù‡Ø§:\\n\"\n",
        "    for original, corrected_word in corrections.items():\n",
        "        corrections_text += f\" - {original} â¤ {corrected_word}\\n\"\n",
        "\n",
        "    return f\"âŒ Ù‚Ø¨Ù„ Ø§Ù„ØªØµØ­ÙŠØ­:\\n{input_text}\\n\\nâœ… Ø¨Ø¹Ø¯ Ø§Ù„ØªØµØ­ÙŠØ­:\\n{corrected_sentence}\\n\\n{corrections_text}\"\n",
        "\n",
        "# Prepare vocab\n",
        "df = df.drop(columns=[\"targe\"], errors=\"ignore\").dropna().drop_duplicates()\n",
        "df[\"text\"] = df[\"text\"].apply(preprocess)\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x: x if len(x.split()) > 5 else None)\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "words_freq = data_vocab(df)\n",
        "\n",
        "# ==== Gradio Interface ====\n",
        "gr.Interface(\n",
        "    fn=lambda x: pipeline(x, words_freq),\n",
        "    inputs=gr.Textbox(lines=5, label=\"Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ\"),\n",
        "    outputs=gr.Textbox(label=\"Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØµØ­Ø­\"),\n",
        "    title=\"ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ© Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\",\n",
        "    description=\"Ø£Ø¯Ø®Ù„ Ø¬Ù…Ù„Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ³Ù†Ù‚ÙˆÙ… Ø¨Ù…Ø­Ø§ÙˆÙ„Ø© ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©.\"\n",
        ").launch()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18727913e7234c73985d864f0c61800a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bc15f7210c141aabfd495a6760b07a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9063ffa00fdb4235adfeb31faee865ac",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6c828f340c494cd78a9e83b0ab813288",
            "value": "â€‡10000/10000â€‡[00:22&lt;00:00,â€‡437.41â€‡examples/s]"
          }
        },
        "2fbd04f25696401db5c16c3c4621759a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c6b179b2eca43d78acc2d67633d8b3a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_18727913e7234c73985d864f0c61800a",
            "value": "Map:â€‡100%"
          }
        },
        "3c6b179b2eca43d78acc2d67633d8b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "531d266653b245ec9988ea19eeb393f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54a0d3e6758b48a7b7c17b24603bc47d",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e9bdc49b57a4d97b52428b7f201ff3b",
            "value": 10000
          }
        },
        "54a0d3e6758b48a7b7c17b24603bc47d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b872f4f2bb4960b48f1b58abceb94d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c828f340c494cd78a9e83b0ab813288": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e9bdc49b57a4d97b52428b7f201ff3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9063ffa00fdb4235adfeb31faee865ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6685aebe8204834b878d145828e5c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fbd04f25696401db5c16c3c4621759a",
              "IPY_MODEL_531d266653b245ec9988ea19eeb393f8",
              "IPY_MODEL_2bc15f7210c141aabfd495a6760b07a7"
            ],
            "layout": "IPY_MODEL_66b872f4f2bb4960b48f1b58abceb94d"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}